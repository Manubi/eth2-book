---
path: "/chapters/consensus"
title: "Consensus"
status: "0"
---

The origins of modern blockchain systems can be traced back to decades-old research in the field of computer science. This history begins with the advent of networked computing in the second half of the 20th century. Prior to the integration of long-distance communication lines into computing hardware, software systems were effectively limited to individual machines or, in rare circumstances, small local clusters of machines. These systems could rely on the tight proximity of various hardware components to reduce any form of communication latency to a mostly insignificant minimum.

The historic ARPANET program, established in 1969, would quickly go on to demonstrate both the viability and utility of large-scale computer networks. Computers connected via ARPANET could communicate with one another at previously unimaginable distances and speeds. It would soon be, in an interesting twist of fate, that a pair of computer worms would make apparent the possibility of not only communication but also collaboration between machines on these networks. The Creeper and Reaper worms, quite possibly the very first computer viruses, jumped between various computers connected to ARPANET and automatically duplicated themselves along the way. Although these worms did no damage to their hosts, they sparked the imagination of researchers interested in making use of idle processing power of machines connected to the ARPANET system.

In 1988, the DEC System Research Center launched the first distributed computing project to be coordinated over the internet. Participants received computing tasks from DEC via email and would return their results to DEC upon completion. Other initiatives, including the famous SETI@HOME project of the University of California at Berkeley, soon followed suit and further simplified the process of communicating tasks and results. With access to the computational resources of an extraordinary number of machines, these efforts massively advanced the state of several key fields of mathematics and computer science.

Simultaneously, other researchers of the time saw potential applications of these networks beyond the pooling of computational resources. Relatively early on in the history of computing, organizations recognized the need for particularly robust computers for use in certain high-value systems. Machines operating in space, for instance, needed to function properly for extended periods of time without any possibility of manual on-board maintenance in the event of an error. Starting in the 1950s, organizations began to develop and utilize so-called "fault-tolerant" computers that had the ability to recover cleanly from significant software and hardware failures. 

Fault-tolerant computing quickly saw wide-spread adoption in critical systems around the world. NASA's CCS fault-tolerant computer found its way into the Voyager spacecraft where, more than forty years later, it continues to operate in the harsh environment of deep space. Military operations, nuclear power plants, and digital financial systems all became natural homes for these error-resistant machines. Fault-tolerant computing was, and continues to be, the foundation of countless computer-based services familiar to us even in the 21st century.

At the heart of fault-tolerant computing sits the principle of replication. When some component of a computer fails, it becomes unable to perform its assigned tasks. If, for example, a memory module crashes, then the data stored on that module becomes inaccessible to other components. A computer could recover from such a failure if the information on the module happens ot be duplicated somewhere else within the system. Fault-tolerant computers are able to fix or bypass errors in individual components by replicating the functionality of these components onto redundant hardware.

In his seminal 1978 paper, "Time, Clocks, and the Ordering of Events in a Distributed System," Leslie Lamport, at the time a researcher at Massachusetts Computer Associates, laid the groundwork for an entirely new model of fault-tolerant computing. Instead of redundant hardware within a single computer, Lamport proposed a form of redundancy produced via communication between many different computers connected to a network.

Lamport demonstrated that this model could be used to accurately replicate "transactional databases" across different machines. Within these databases, each database update is represented as an individual event, or "transaction." The final state of a transactional database is then computed as the result of executing each of these transactions in order. If several computers could come to agreement about the ordering of transactions within such a system, Lamport argued, each computer could accurately reconstruct the final system for itself.

Lamport further envisioned the possibility that entire computers be replicated the same way. Under the "state machine" model, computers can be represented, in an abstract sense, as entities that store some past information in memory (the "state") and may modify this information according to a given set of operations. Just as with a transactional database, a state machine could be reconstructed if the list of operations it executed were recorded and played back in exactly the same order. Perhaps, therefore, a failure on one computer could be circumvented if execution could seamlessly continue on another.

Research into this state machine model of fault tolerance had to be carried out under some set of assumptions about the system itself. Early work was often restricted to systems in which computers were expected to only experience so-called "crash-faults," sometimes also known as "fail-stop faults." Within this framework, computers would either perfectly follow some prescribed protocol or cease to function entirely. Computers on the network would only behave "benevolently" and would not, for instance, attempt to forge messages from other machines.

Designs operating under this assumption could, as we'll shortly discuss, take advantage of some convenient simplifications. However, this mental framework not only reduced the problem space for researchers, but also generally reflected the needs of many real-world applications. Organizations making use of such systems internally, perhaps to maintain backups of some important database or computing process, could typically ignore the possibility of actively malicious behavior from their own machines. As a result, this approach quickly grew in popularity and has held traction well into the 21st century.

We now direct this chapter into a more detailed analysis of the construction of fault-tolerant distributed computing systems. We begin this study with definitions of certain key concepts used in the specifications of these systems. Exact terminology employed within this book is, whenever possible, faithful to original language of "the literature," and may occasionally rather poorly summarize underlying concepts. We attempt to provide additional context and exposition when we find this to be the case.

Our brief history of fault tolerance thus far has referred in passing to concepts like "computers" and "networks," among others. These are, in practice, highly technical and varied elements. Computers can be constructed in countless ways and still achieve the goal of "computing." One can verify this statement within the span of a brief journey to the nearest electronics store. Physical network connections are similarly diverse in both form and function. It's for these reasons that the field of computer science has developed useful conceptual abstractions that smooth out differences between real components.

Computers are, unsurprisingly, the foundation of computational systems. However, it's not necessary that the entities interacting within a networked system operate on distinct hardware. Indeed, computer hardware has been capable of running multiple processes in parallel since the mid mid-20th century. Computer scientists therefore often refer to "nodes" as abstractions of computers on a network. Nodes can be thought of as unique entities on a network that have some computational processing power and access to local memory storage. 

Communication lines between computers are similarly abstracted to remove any notion of an underlying physical medium. Whether information is transferred via electromagnetic waves, sound waves, or even carrier pigeon makes no difference to our analysis. We instead depict "logical" connections between nodes, over which nodes can send distinct packets of information called "messages." We further assume that some unknown amount of time must elapse between the moment that a message is sent and the moment at which it is received. 

Lines of communication are necessary but not sufficient for collaboration between nodes. Just as humans need languages, nodes must share a vocabulary of sorts in oder to successfully send and receive information. These communication "protocols" allow nodes to understand the meaning of messages on the network. We generally do not define any specific communication protocol in theoretical network studies, though we do assume that one exists. 

We also use the term "protocol" to refer to the set of actions nodes in a system are expected to take at any given time. Since node behavior defines the overall properties of a system, it's this protocol that we must now construct. Our primary goal, as previously noted, is to ensure that nodes be able to come to agreement about a particular set and ordering of events. We therefore come to call this construction a "consensus protocol."

Let us briefly restate our problem so that we can analyze some initial barriers. We have a distributed system of `N` nodes. Our goal is to develop some protocol that guarantees the nodes will, after some amount of time, come to agreement about a particular ordered list of "events." Active nodes must be able to come to agreement even when other nodes may be faulty. At this point, we are only interested in making our protocol resistant to crash faults. That is to say, nodes either follow the protocol perfectly or do not function at all. 

Though such a system needs to be maximally resistant to faults, there is a theoretical limit to the number of faults we can handle. We would clearly have a problem achieving much of anything if 100% of nodes were to become faulty. Research into this problem found that these crash-fault tolerant systems could sustain up to `n/2 + 1` failures. We often represent this number alternatively through the lens of the total number of nodes necessary given a certain number of faulty nodes, `f`, as `2f + 1`. One intuitive way of understanding this boundary is that if 50% of nodes were allowed to become faulty, then we could split the network into two segments that simply think the other half is faulty and may continue to operate in conflict.

The basis of a crash-fault tolerant system is that a certain threshold of nodes must come to agreement about an operation before the operation is actually executed. One of the simplest methods for reaching this agreement is to hold a vote. Conveniently, since we do not expect malicious behavior from our nodes, we can assign a single node to manage the entire voting process without fear of tampering. This node will send out events to vote on, tally up votes on particular events, and notify nodes of any relevant results. Our only core requirement is that nodes will never execute an operation unless `n/2 + 1` nodes have voted to do so.

Another way to look at this `n/2 + 1` requirement is to imagine the following scenario. Let's say that `n/2 + 1` nodes voted for event `x` to be `y`. Now assume that at a later time, `2/n + 1` nodes voted for event `x` to be `z`, a contradiction of the previous vote. In order for this to have happened, `n` nodes must have made `n + 2` votes, meaning at least one node made a contradictory vote. Since nodes cannot make contradictory votes under the crash-fault model, we can safely assume that this could never happen. Therefore, once `n/2 + 1` nodes vote for a particular event, the system will never include a contradictory vote.

This model of fault-tolerance through voting between machines is still extremely popular today. Services like Apache ZooKeeper and etcd have vastly simplified the process of deploying these systems. Many large organizations use crash-fault tolerant networks to store important information.

Although crash-fault tolerant systems are heavily used in production applications, they sometimes prove insufficient for the task at hand. Smaller projects may be able to assume that nodes will always behave according to protocol. However, once a system becomes highly valuable, the attack landscape shifts. Perfect protocol implementations may prevent unintended node behavior, but they do not prevent an attacker from gaining access to the machines on which nodes are running. Even with strong security practices, almost any system could be compromised, at least to some extent, by a malicious adversary.

It's possible to use simple crash-fault tolerance in the face of this problem, though this introduces less-than-ideal scenarios. For instance, a system could recover from an attack by reverting to a previous state. Of course, this relies on the existence of backups not yet compromised by the attacker and additionally results in a potentially massive service disruption. Research effort was, as a result, expended in the search for protocols that could provide native resistance to the presence of malicious nodes. This problem proved difficult enough to spur the development of an entirely new field of research.

The problem space for systems robust to actively malicious nodes was summarily defined in a famous 1982 paper entitled, "The Byzantine Generals Problem." Within this work, Robert Shostak, Marshall Pease, and none other than the aforementioned Leslie Lamport present an entertaining abstraction of the problem in the context of a Byzantine army surrounding a large city. The author's rendition of the underlying elements in such a network became so strongly connected to the space that resulting protocols came to be known as "Byzantine fault tolerant."

"The Byzantine Generals Problem" describes a fictional scenario in which a Byzantine army is attempting to execute a military maneuver around a vast city. The army is broken out into divisions, each of which is controlled by a general. Some of these generals are "loyal" and will always follow their prescribed protocol, while others are "traitors" and will attempt to actively manipulate the system to the greatest extent possible. The paper attempts to develop a protocol that allows loyal generals to agree on a unified battle plan, even in the presence of traitors.

The authors give a few more constraints to further flesh out the problem. Generals communicate with one another via messenger, so we can clearly expect some time to elapse between send and receipt. One of the generals is assigned as the "commanding general" and is responsible for sending out initial orders. All other generals are referred to as "lieutenant generals." Any general, whether a lieutenant general or the commanding general, may be a traitor. Given this additional context, the exact problem statement from the paper is:

A commanding general must send an order to his `n - 1` lieutenant generals such that:
1. All loyal lieutenants obey the same order.
2. If the commanding general is loyal, then every loyal lieutenant obeys the order he sends.

Under this more challenging model, traitors can, for instance, lie about the orders they received in an attempt to trick loyal generals into acting on conflicting orders. Unfortunately, this reduces the number of faults the system can sustain in comparison to crash-fault tolerant systems. The authors demonstrate that with three generals, a single traitor can cause a total failure. Let's take a look at a sketch for this proof.

A three-general army has one commanding general and two lieutenant generals, here depicted as `c`, `l1`, and `l2`. If our traitor is a lieutenant general, then the loyal lieutenant may receive conflicting orders:

However, the loyal general may see the same exact set of messages if the commander is the traitor:

Since `l1` sees the same messages in both cases, there's no way for `l1` to determine which of the generals is a traitor. This holds, without less of generality, for `l2`. As a result, the loyal generals are unable to find one another and come to any agreement about the correct order to execute.

The paper later shows that if there are `n` traitors, there must be at least `2n + 1` loyal generals for the system to function properly. This requirement stems from the lack of a trusted "coordinator" available to make generals aware of their own orders and orders given to others. Previously, in our discussion of crash-fault tolerance, we assigned a node to manage the voting process, something quite similar to the commanding general in this new scenario. However, as the commanding general may be a traitor, loyal lieutenants cannot simply trust the commander's messages.

Instead, the lieutenants must communicate directly with other generals in order to make a decision. The paper describes two solutions that both rely on this sort of direct communication between generals. For now, we'll assume that a general may communicate directly with any other general, i.e., the generals and their communication lines form a completely connected graph. Each solution effectively requires that, upon receiving an order from the commander, lieutenant generals share their orders with all other generals.

The difference between the two presented solutions lies in the use of cryptographic signatures in the second. These signatures make it impossible for a traitor to forge a message from another general. Most modern systems use this technique, so we'll explore the basic elements of this solution now. We follow a two-step process in which the commander first distributes orders to lieutenants, and lieutenants then share their received orders with all other generals. 

If the commanding general is loyal, this protocol can relatively easily find a common order for loyal generals. Since traitors cannot forge an alternative order, the loyal lieutenants will only see a single order given by the commander. With only one possible order, the lieutenants will end up executing the same action.

If the commander is a traitor, more formal reasoning is necessary to see the correctness of the system. It's possible that a general sees two conflicting orders from the commander. This general now knows that the commander is a traitor, but must still come to agreement with all other loyal lieutenants. We handle this scenario with a deterministic function that defines how loyal lieutenants should act when given a list of possible actions. We can therefore still execute a common order as long as we can guarantee that all loyal generals receive the same list of possible orders.

A traitorous commanding general could coordinate with other traitors to attempt to give loyal generals different sets of orders. For instance, the traitors could choose to only send the conflicting order to half of the loyal generals. Without further communication, the loyal generals would not act in unison. We can provide a simple solution by requiring that loyal generals relay conflicting orders to all other generals. After this second round of communication, all loyal generals are guaranteed to share the same information. The paper shows that we can achieve the same result if loyal generals only relay orders with less than `f` votes, where `f` is the maximum number of traitors that can be safely handled by the system.

One way to understand the `3f + 1` general requirement for `f` traitors is again through the lens of a "threshold" for carrying out an action. In the context of crash-faults, we needed a threshold of `n - f` votes in order to make a decision. Since crashed nodes could not vote, we could derive that all of the `n - f` votes came from properly functioning nodes. When the faulty nodes, in this cause our traitorous generals, can actually contribute to the vote, we must increase the threshold to effectively discount the possibility of malicious votes. 

Many improved versions of the protocol described in "The Byzantine Generals Problem" have been published in the decades since the paper's debut. However, the core elements of the protocol, including the use of cryptographic signatures and peer-to-peer messaging have remained largely the same. The ability for these systems to remain operational in the face of malicious nodes have made them particularly useful for critical systems in healthcare and finance, where disruptions could cause massive and long-lasting damage to society.

Even with these advantages, BFT systems remained a niche subject until a novel reimagination in the late 90s and early 00s changed the field forever. BFT protocols along the lines of those proposed in "The Byzantine Generals Problem" made it possible for machines connected over the internet to share a "log of events." Their use among financial institutions made obvious that such protocols could potentially underpin a digital money system. Perhaps this money system could even live exclusively within the network and remain entirely free of attachments to traditional state-operated currencies.



Although the underlying technology of byzantine fault tolerance was agnostic to the final use-case, as best demonstrated by the distributed state machine, digital money seemed to be both useful and relatively feasible at the time. Basic monetary systems would effectively require that nodes share a minimal ledger of currency transfers between users. A lightweight version of this would basically only require that nodes do some simple math for each new transfer. This core functionality alone was not particularly novel or difficult to achieve.

However, money is also a fascinating social construction that tends to carry strong and varied meanings. Proponents of the early digital money movement generally felt that such a system should be not only robust to attackers, but also open to all new participants. TO string an analogy to the Byzantine Generals problem, these individuals wanted to create a system in which anyone could become a general at any time. Primarily, the aim of such a system was to circumvent the potential problems inherent when the nodes in control of a money system are fixed and unchanging. 

This model of open access was not immediately compatible with traditional byzantine fault tolerant protocols. One fundamental problem was that an attacker could create virtually unlimited identities on the network, pretending to be different unique participants. Without a method to clearly assign an identity to a verifiable human, there was no apparent way to prevent this attack. To this day, the problem of assigning unique digital identities without the need for a third-party (e.g. passport assigned by the U.S. government) remains an extremely difficult and, by many standards, unsolved problem.

A solution to this dilemma was found in research related to the prevention of email spam. In fact, spam prevention is, in many ways, a challenge directly related to the prevention or detection of "sybil identities" created by malicious spam producers. The basic mechanism employed here was to require that the sender perform some sort of verifiable computational effort in order to send a message. This effort had to be verifiable in the sense that the recipient could easily confirm to a high degree of certainty that the effort had been performed. 

The theoretical basis of such a system was not to perfectly assign digital identities to real people, but to significantly increase the cost of spam email. If a single email expended 1c of electricity, then an attacker would quickly rack up quite the bill to send thousands or even millions of emails. Such a system could even be tuned by the user to remove or reduce costs for whitelisted addresses.

Researchers quickly realized that a similar approach could enable "indirect" identity verification on a digital money system. Instead of needing explicitly assigned identities, users could participate in consensus by expending computational effort. The weight of a user's influence over the system would be measured not by their number of identities but by their expended computational effort. As a result, an attacker could no longer gain an advantage by creating multiple identities.

Even if such a system could manage to skirt the need for explicit identities, it would face further dilemmas. Traditional BFT protocols called for some node to act as the "leader" and to propose a change in the network. Since participants were known in advance, these protocols would explicitly assign nodes to act as leaders, usually for some fixed period of time. Without known participants, the digital money protocols needed some new way to determine a leader at any given point in time.

These systems settled on a model that satisfied the goal to attach influence to expenditure of computational resources. Participants would effectively participate in a continuous lottery system to determine the next leader. During each round of the lottery, participants would repeatedly search for some given target hash. The random nature of the hashing process guaranteed that, on average, a user's odds of winning are proportional to their percentage of total resource expenditure.

However, this lottery system could not simply assign the winner as leader for a certain amount of time without also introducing the possibility that the winner's proposed changes go entirely ignored. Instead, he winner would actually perform their work on a reference to a specific set of proposed changes, and would publish these changes along with their winning lottery ticket. It's important to note here that proposed changes necessarily had to point to the state being changed. This prevents, for instance, two conflicting changes from being executed on the same state. However, this presents an interesting problem in that it's possible for two users to win the same lottery. In this case, participants may have different views of the network. Generally, the less amount of time the lottery takes, the more likely that we have multiple winners. It therefore makes sense to extend the lottery time to reduce the chance of having multiple winners. When the lottery time is high, we want to put many changes within a single proposal or the system would be unbearably slow. As a result, we get blocks of transactions.

This problem of multiple leaders at the same time introduces the concept of forks in the network. When two users create a block at the same time, it's necessary for the rest of the network to figure out which block to build on. We need to decide on a single block because new blocks must reference some specific state as outputted by a block. Participants follow a "fork choice rule" to figure out what to do when presented with more than one block for the same height. A fork choice rule is an algorithm that picks a chain from a list of potential forks, based on some metric.

Within Proof of Work, the simplest for choice rule is the longest chain rule.

[LCR EXPLAINER]

So far we've remained intentionally vague about the contents of the proposals that these participants are making. Traditional distributed systems could be used for any number of use cases from backing up important information to ensuring that critical software stays operational. Of course, some of our direction on this has already been covered as the original intention was to build a digital money system. As we previously noted, we actually need some incentive for our participants, which can be paid out via our own digital money system.

At this point, we just need to figure out how we want to represent such a system. Early projects like Bitcoin made use of an interesting transactional model called the "unspent transaction output" or "UTXO." Under this model, all currency on the platform would be represented as unique "bundles" of coins that could be combined to broken apart. Transactions would take up to a certain number of these bundles as "inputs" and spit out any number of "outputs." These resulting outputs could then be used as inputs to further transactions.

Within Bitcoin specifically, coins can only enter the system through the mining process. The creator of a block includes a special transaction that has no inputs but one output with a value equal to the block reward. This is the only instance in which the output amount can be greater than the input amount. For all other transactions, input amount must be greater than or equal to output amount. Any difference between input and output amounts is automatically considered to be a transaction fee paid to the miner.

Although this model is not particularly intuitive, it does provide certain useful properties. Since a user can change their address between each transaction, their transactions become more difficult to trace. Someone would need to do a relatively extensive analysis of the graph of outputs in order to study a user's behavior. Furthermore, outputs are cleanly separated from one another in that transactions with no shared inputs can be executed simultaneously. This makes the model quite parallelizable.

This sort of system is usually compared to the "account" model of blockchains like Ethereum. Account-based systems are generally more familiar to us than UTXOs. Under this model, users can create accounts that hold their funds. Transactions between accounts are effectively debits from one account and credits to another, much like a traditional bank account. This is somewhat simpler than the UTXO model but can also be more efficient in many cases.
